* how machine learning is used at a hedge fund
	* in general the focus is on creating a model that can be used to predict future prices towards stock or other assets
	* machine learning provides a suite of tools that supports a data centric way to build predictive models
	* the ML problem
		* scientists talk about algorithms in terms of the problems they solve
		* the problem machine learning algorithms solve...
			* in most cases machine learning programs are focused on building a model 
				* a model: something that takes in observations (of the world), runs them through some sort of process, provides an output (typically a prediction)
					* example: the observations are some features of stocks, the prediction is a future price 
					* the observations can be multi dimensional; i.e., there might be multiple factors that we're considering (Bollinger bands, p/e ratio, etc.)
					* the prediction is typically single dimension 
				* in machine learning we are trying to use data
					* the machine learning process:
						* take historical data, run it through a machine learning algorithm of some sort to generate/learn a model, then at run time (when we need to 
						use the model) we pass in observations and get outputted predictions -- the model maps these observations to predictions 
	* supervised regression learning
		* "regression" -- in this case it means that we are trying to make a numerical approximation/prediction (as opposed to classification learning where we might be trying 
		to classify an object into one of several types) 
		* "supervised" -- in this case it means that we provide the machine with many, many axample observations and example output predictions -- that's how it learns the 
		associations between observations and predictions 
		* "learning" -- in this case means training system via/with data
		
		* there are a lot of algorithms that solve this problem and are supervised learning techniques 
			* linear regression (parametric) -- a method that finds parameters for a model and discard historic data
			* k nearest neighbor (knn) (instance based) -- extremely popular approach, keep all historic data that observations and predictions pairs and when it's time to 
			make a prediction we consult that data (the observation/prediction pairs) finds the k nearest neighbors to its current observation and looks what they say
			essentially a vote -- that's what makes it instance based
			* decision trees -- store a tree structure and when a query comes in, it bounces down that tree according to factors of the data, each node in the tree represents
			a question ("is that observation greater than x?"), eventually we reach a leaf and that is the regression value that is returned
			* decision forest -- lots and lots of decision trees taken together, query each one to get an overall result
			
	* how it works with stock data
		* take stacks of pandas stock dataframes (differing features like bollinger bands, etc.) 
		* look at values of our features at a past date
		* look n days into future at the stock's historical price (a day that has already occured) 
		* this yields a pairing of features with a prediction, save this instance of data
		* move forward
		* reach a point where there are no more predictions data (no more recorded prices)
		* now have data to run through the algortihm to learn a model 
	* example process
		1. select x1, x2, x3 ... features (predictive factors aka measurable quantities about a company that can be predictive of its stock price) 
		2. select prediction (usually we want to select change in price, market relative change in price, future price) 
		3. feature/prediction pairs become our data
		4. consider depth and breadth of data we are going use to train the system with (time period aka how far back in time to go back to train system, stock universe aka 
		symbols)
		5. train -- can take this data to train our model; i.e., unleash our machine learning algorithm which takes the data and converts it into a model  
		6. predict -- measure the quantities about the stocks we want to make a prediction for now, measure observations of today plug into model, model will provide a prediction

	* backtesting
		* how accurate our these forecasts? can you act on them? 
			* a first step answer can be found by backtesting
				* i.e., roll back time and test your system
					* in order to test capabilities of our approach, we have to limit the data it sees to a certain amount of time and then make predictions into the
					simulated future
				
	* problems with regression
		* performance in the real world not as "awesome" as in backtesting
		* problems:
			* noisy and uncertain -- value but has to accumulated over many trading opportunities
			* challening to estimate confidence -- hard to know how confident you should be in a forecast -- if you did know you could bet less on forecasts that are less
			certain
			* holding time and allocation -- not clear on how long you should hold a position that might have arisen from a forecast and how you should have allocated to that
			position
		* alternative to regression -- policy learning
			* reinforcement learning -- instead of making a forecast of a future price, system learns a policy and a policy tells a system whether to buy or sell a stock
	* problems we will focus on
		* look a certain period of data
		* train model over that period
		* make forecasts and trade over some other period
		* implement several ML algorithms to create different models, we will compare them one against the other
		* test another time period beyond our data time period, this will become our observations
		* pass those observations through our model to create a prediction
		* generate an orders.txt file from forecast/prediction
	

* Regression
	* aka numerical model -- using data to build a model that predicts a numerical output based on a set of numerical inputs (observations)
	* parametric regression -- way of building a model where we represent the model with a number of parameters	
		* example: predict rainfall amount based on barometric pressure changes 
			* usually as barometric pressure declines it indicates that "bad" weather is coming (i.e., rain) 
			* usually as barometric pressure increases it indicates that that "good" weather is coming (i.e., less rain) 
			* data represented on scatter plot -- each dot represents a day with change in barometric pressure (x axis)  and the amount of rainfall for that day (y axis)
			* we'd like to create a model based on a set of data where when we query the model at any particular point it will give a prediction of how much it will rain
				* measure that point's barometric pressure and estimate its rainfall 
			* classic solution is to fit a line to the data -- aka LINEAR REGRESSION
				* the model looks like -- y = mx + b
					* x -- barometric pressure change 
					* m && b -- the parameters of our model
						* our model is now fully described by these 2 parameters and if we want to estimate/query how much it will rain at any particular point,
							* we measure the barometric pressure, plug that measurement into our model (as the x variable) 
								* solve the y = mx + b equation to the get the y axis (aka the amount of rainfall for that point)
					* the linear regression approach is how we arrive at m and b 
				* linear regression does not track the actual behavior of the data -- we can make a more complex model -> parametric regression 
		* parametric regression
			* instead fitting a line to data, we can fit a polynomial to the data (add one more term) 
				* y = ax^2 + mx + b	-- model now represented by 3 parameters (a, m, b) 
				* in general these parametric approaches come away with a number of parameters -- the more complex the model, the more parameters
				* in the end we throw away the data and the model is represented by just the parameters
	* k nearest neighbor (knn) regression
		* data centric (aka instance based) approach  
		* keep the data and use it when we make a query
		* example: barometric pressure and rain
			* select a query (change in barometric pressure for a given day)
				* assume query = -5mm; assume k = 3
			 		* therefore, we will find the 3 nearest historical data points to this query 
						* we will then use those 3 nearest neighbors to estimate how much it will rain
							* i.e., we take the mean of their y values (i.e., their rainfall amounts) 
	* kernel regression
		* give a weight to each of the contributions of the nearest neighbor data points according to how distant they are  

	* parametric vs non parametric
		* parametric -- biased: we have an initial guess of what the form of the equation/solution is 
			* pros: we don't have to store the original data -- space efficient -- querying is fast
			* cons: we can't easily update the model as more data is gathered -- usually have to do a complete re run of the ML algorithm to update model
				-- training is slow
		* non-parametric (instance based) -- unbiased:  
			* pros: new data/evidence/observations can be added easily since no parameters need to be learned, adding data points doesn't consume additional
			time -- training is fast
			* cons: we have to store all the data points -- it's hard to apply when we have a huge data set -- querying is slow
			* avoid having to assume a certain type of model (linear, quadratic, etc.) -- suitable to fit complex patterns where we don't really know what 
			the underlying model is like 

	* training and testing
		* assume we have some set of features (i.e., multi dimensional observations) and we want to get a prediction outputted
			* in order to evaluate our learning algorithms in a scientific manner, we need to split this data into at least 2 sections: training section, testing section
			* if trained over the same data that we tested over, the results would be suspicious because we should obviously do very well if we test over the same data we train on
				* this procedure of separating testing and training data is called Out of Sample Testing -- a very important and essential technique
			* 































